name: External - OpenShift Agent-Based Installer Helper
on:
  workflow_dispatch:
    inputs:
      runner_tag:
        description: 'Runner'
        required: true
        default:  'w2tdd-github-runner' #'changeme-github-runner'
      folder_name:
        description: 'Supported Deployment Configurations'
        required: true
        default: 'stretched-metro-cluster'
        type: choice
        options:
          - bond0-single-bond0-vlan
          - stretched-metro-cluster
          - cnv-bond0-tagged
          - cnv-single-bond0-vlan
          - converged-bond0-signal-vlan
          - sno-bond0-signal-vlan
      guid:
        description: 'GUID'
        required: true
        default: 'w2tdd' #'12345'
      zone_name:
        description: 'ZONE_NAME'
        required: true
        default: 'sandbox2168.opentlc.com' #'sandbox000.opentlc.com'
      target_server:
        description: 'target_server'
        required: true
        default: rhel9-equinix
      action:
        description: 'action'
        required: true
        default: create
        type: choice
        options:
          - create
          - delete
      deploy_openshift:
        description: 'Deploy The OpenShit Cluster on Launch'
        required: true
        default: true
        type: choice
        options:
          - true
          - false
      deployment_tag:
        description: 'OpenShift Version'
        required: true
        default: '4.17'
        type: choice
        options:
          - '4.15'
          - '4.16'
          - '4.17'
      disconnected_install:
        description: 'Enable Disconnected Install'
        required: true
        default: false
        type: choice
        options:
          - true
          - false
      community_version:
        description: 'Use community version of software (true/false)'
        required: true
        default: 'false'
        type: choice
        options:
          - true
          - false

env:
    TARGET_SERVER: ${{ inputs.target_server }}
    VM_NAME: "openshift-agent-install"
    ACTION: ${{ inputs.action }}
    DEPLOY_OPENSHIFT: ${{ inputs.deploy_openshift }}
    LAUNCH_STEPS: ${{ inputs.auto_launch_steps }}
    TAG: ${{ inputs.deployment_tag }}
    DISCONNECTED_INSTALL: ${{ inputs.disconnected_install }}
    FOLDER_NAME: ${{ inputs.folder_name }}
    COMMUNITY_VERSION: ${{ inputs.community_version }}
    RUNNER: ${{ inputs.runner_tag }}
    GUID: ${{ inputs.guid }}
    ZONE_NAME: ${{ inputs.zone_name }}

jobs:
  ansible-role-update-ip-route53-requirements-config:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    if: ${{ inputs.action == 'create' }}
    steps:
      - uses: actions/checkout@v4

      - name: updating route 53 record
        run: |
          cat >/tmp/requirements.yml<<EOF
          ---
          collections:
            - amazon.aws
          roles:
            - name: ansible_role_update_ip_route53
              src: https://github.com/tosin2013/ansible-role-update-ip-route53.git
              version: master
          EOF

          ansible-galaxy install -r /tmp/requirements.yml --force -vv
          pip3 install boto3 botocore

  download-openshift-agent-install:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    if: ${{ inputs.action == 'create' }}
    needs: ansible-role-update-ip-route53-requirements-config # Ensure the previous job runs first
    steps:
      - uses: actions/checkout@v4

      - name: Deploying OpenShift Agent-Based Installer
        run: | 
          if [ -d "/opt/openshift-agent-install" ]; then
            cd /opt/openshift-agent-install
            sudo git config pull.rebase false
            sudo git pull
          else
            cd /opt/
            sudo git clone https://github.com/Red-Hat-SE-RTO/openshift-agent-install.git
          fi


  configure-ansible-role-update-ip-route53:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    if: ${{ inputs.action == 'create' }}
    needs: download-openshift-agent-install # Ensure the previous job runs first
    steps:
      - uses: actions/checkout@v4

      - name: updating route 53 record
        run: |
          CLUSTER_FILE_PATH="/opt/openshift-agent-install/examples/${{ env.FOLDER_NAME }}/cluster.yml"
          CLUSTER_NAME=$(yq e '.cluster_name' ${CLUSTER_FILE_PATH})
          IP_ADDRESS=$(hostname -I | awk '{print $1}')
          echo "Setting DNS Records below"
          echo "api.${CLUSTER_NAME}.${{ env.GUID }}.${{ env.ZONE_NAME }}"
          echo "*.apps.${CLUSTER_NAME}.${{ env.GUID }}.${{ env.ZONE_NAME }}"
          # Decrypt the vault file to access AWS credentials
          if sudo grep -q '$ANSIBLE_VAULT;1.1;AES256' "/opt/qubinode_navigator/inventories/${{ inputs.target_server }}/group_vars/control/vault.yml"; then
            echo "The file is encrypted with Ansible Vault. Decrypting the file..."
            sudo -E /usr/local/bin/ansiblesafe -f "/opt/qubinode_navigator/inventories/${{ inputs.target_server }}/group_vars/control/vault.yml" -o 2
            if [ $? -eq 0 ]; then
              echo "File decrypted successfully."
            else
              echo "Failed to decrypt the file."
            fi
          else
            echo "The file is not encrypted with Ansible Vault."
          fi

          # Extract required AWS credentials using yq
          AWS_ACCESS_KEY_ID=$(sudo -E yq eval '.aws_access_key' "/opt/qubinode_navigator/inventories/${{ inputs.target_server }}/group_vars/control/vault.yml")
          AWS_SECRET_ACCESS_KEY=$(sudo -E yq eval '.aws_secret_key' "/opt/qubinode_navigator/inventories/${{ inputs.target_server }}/group_vars/control/vault.yml")

          # Re-encrypt the vault file
          sudo -E /usr/local/bin/ansiblesafe -f "/opt/qubinode_navigator/inventories/${{ inputs.target_server }}/group_vars/control/vault.yml" -o 1

          cat >/tmp/playbook.yml<<EOF
          - name: Populate OpenShift DNS Entries
            hosts: localhost
            connection: local
            become: yes

            vars:
              update_ip_r53_aws_access_key: "${AWS_ACCESS_KEY_ID}"
              update_ip_r53_aws_secret_key: "${AWS_SECRET_ACCESS_KEY}"
              use_public_ip: true
              private_ip: "${IP_ADDRESS}"
              update_ip_r53_records:
                - zone: "${{ env.ZONE_NAME }}"
                  record: "api.${CLUSTER_NAME}.${{ env.GUID }}.${{ env.ZONE_NAME }}"
                - zone: "${{ env.ZONE_NAME }}"
                  record: "*.apps.${CLUSTER_NAME}.${{ env.GUID }}.${{ env.ZONE_NAME }}"

            roles:
              - ansible_role_update_ip_route53
          EOF

          if [ "${{ env.ACTION }}" != "delete" ]; then
            sudo -E ansible-playbook /tmp/playbook.yml -v || exit $?
          fi

  configure-kcli-profiles:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 30
    needs: configure-ansible-role-update-ip-route53 # Ensure the previous job runs first
    steps:
      - uses: actions/checkout@v4

      - name: Configure kcli profiles
        run: | 
          echo "Running on ${{ inputs.runner_tag }}..."
          export TARGET_SERVER=${{ env.TARGET_SERVER }}
          export VM_PROFILE=freeipa
          export COMMUNITY_VERSION=${{ env.COMMUNITY_VERSION }}
          export EMAIL=${{ env.EMAIL }}
          if [ "${{ env.VM_NAME }}" != "freeipa" ]; then
            export CUSTOM_PROFILE=true
          else 
            export CUSTOM_PROFILE=false
          fi
          if [ -d "/opt/kcli-pipelines" ]; then
            cd /opt/kcli-pipelines
            sudo git config pull.rebase false
            sudo git pull
          else
            cd /opt/
            sudo git clone https://github.com/tosin2013/kcli-pipelines.git
          fi
          sudo -E /opt/kcli-pipelines/configure-kcli-profiles.sh || exit $?

  deploy-dns-server:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    if: ${{ inputs.action == 'create' }}
    needs: configure-kcli-profiles # Ensure the previous job runs first
    steps:
      - uses: actions/checkout@v4

      - name: Deploying OpenShift Agent-Based Installer
        run: |
          export VM_NAME=freeipa
          export VM_PROFILE=freeipa
          export ACTION=${{ env.ACTION }}
          export COMMUNITY_VERSION=${{ env.COMMUNITY_VERSION }}

          CLUSTER_FILE_PATH="/opt/openshift-agent-install/examples/${{ env.FOLDER_NAME }}/cluster.yml"

          if [ -n "${{ env.ZONE_NAME }}" ]; then
            export DOMAIN="${{ env.GUID }}.${{ env.ZONE_NAME }}"
            sudo -E yq e -i ".domain = \"$DOMAIN\"" /opt/qubinode_navigator/inventories/"${{ inputs.target_server }}"/group_vars/all.yml
            sudo -E yq e -i ".base_domain = \"$DOMAIN\"" "${CLUSTER_FILE_PATH}"
            sudo -E yq e -i ".dns_search_domains[0] = \"$DOMAIN\"" "${CLUSTER_FILE_PATH}"
            sudo -E yq e -i "del(.dns_search_domains[1])" "${CLUSTER_FILE_PATH}"
          else
            DOMAIN=$(sudo -E yq eval '.domain' "${ANSIBLE_ALL_VARIABLES}")
          fi

          if [ "${ACTION}" == "create" ]; then
            cd /opt/kcli-pipelines/
            ./deploy-vm.sh
          fi

  deploy-vyos-router:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    if: ${{ inputs.action == 'create' }}
    needs: deploy-dns-server # Ensure the previous job runs first
    steps:
      - uses: actions/checkout@v4

      - name: Deploy Vyos Router
        run: |
          export TARGET_SERVER=${{ env.TARGET_SERVER }}
          export VM_NAME=vyos-router
          export VM_PROFILE=vyos-router
          export ACTION=${{ env.ACTION }}
          export COMMUNITY_VERSION=${{ env.COMMUNITY_VERSION }}
          if ! sudo kcli list vms | grep -q ${VM_NAME}; then
              if [ "${{ env.ACTION }}" == "create" ]; then
                  cd /opt/kcli-pipelines/
                  sudo -E ./deploy-vm.sh
              else
                  echo "Skipping deployment as vyos-router VM does not exist and ACTION is not create"
              fi
          else
              echo "vyos-router VM exists, skipping deployment"
          fi

  wait-for-vyos-router:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    if: ${{ inputs.action == 'create' }}
    needs: deploy-vyos-router # Ensure the previous job runs first
    steps:
      - uses: actions/checkout@v4

      - name: Initialize Variables
        run: |
          echo "IP_ADDRESS=192.168.122.2" >> $GITHUB_ENV
          echo "MAX_WAIT_TIME=1800" >> $GITHUB_ENV  # 30 minutes in seconds
          echo "WAIT_INTERVAL=300" >> $GITHUB_ENV   # 5 minutes in seconds

      - name: Wait for Router to be Accessible
        continue-on-error: true # Allow the step to continue even if the script fails
        run: |
          start_time=$(date +%s)
          end_time=$((start_time + $MAX_WAIT_TIME))

          echo "Waiting for $IP_ADDRESS to be accessible..."

          while true; do
            if ping -c 1 "$IP_ADDRESS" > /dev/null 2>&1; then
              echo "Router is accessible now. Continuing..."
              exit 0
            else
              current_time=$(date +%s)
              remaining_time=$((end_time - current_time))

              if [ $remaining_time -gt 0 ]; then
                echo "Router is not accessible yet. Please access this page to manually configure the router: https://github.com/tosin2013/demo-virt/blob/rhpds/demo.redhat.com/docs/step1.md"
                echo "Remaining time: $((remaining_time / 60)) minutes"
                sleep "$WAIT_INTERVAL"
              else
                echo "Timeout reached. Router is still not accessible."
                exit 0  # Exit with non-error code to avoid immediate failure of the job
              fi
            fi
          done

      - name: Validate Router Deployment
        if: ${{ success() }}
        run: |
          if ! ping -c 1 $IP_ADDRESS > /dev/null 2>&1; then
            echo "Router deployment failed. Manual intervention required."
            exit 1 # Fail only if the router is not accessible at this point
          else
            echo "Router is accessible."
          fi

  configure-vyos-router:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    if: ${{ inputs.action == 'create' }}
    needs: wait-for-vyos-router # Ensure the previous job runs first
    steps:
      - uses: actions/checkout@v4

      - name: Configure Vyos Routes
        run: |
          #!/bin/bash

          # Define the list of networks
          networks=(
              "192.168.49.0/24"
              "192.168.50.0/24"
              "192.168.51.0/24"
              "192.168.52.0/24"
              "192.168.53.0/24"
              "192.168.54.0/24"
              "192.168.55.0/24"
              "192.168.56.0/24"
              "192.168.57.0/24"
              "192.168.58.0/24"
          )

          gateway="192.168.122.2"

          for net in "${networks[@]}"; do
              if ! ip route show | grep -q "$net"; then
                  sudo ip route add "$net" via "$gateway"
                  echo "Route for $net added."
              else
                  echo "Route for $net already exists."
              fi
          done

  wait-for-vyos-route:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    if: ${{ inputs.action == 'create' }}
    needs: configure-vyos-router # Ensure the previous job runs first
    steps:
      - uses: actions/checkout@v4

      - name: Initialize Variables
        run: |
          echo "IP_ADDRESS=192.168.50.1" >> $GITHUB_ENV
          echo "MAX_WAIT_TIME=1800" >> $GITHUB_ENV  # 30 minutes in seconds
          echo "WAIT_INTERVAL=300" >> $GITHUB_ENV   # 5 minutes in seconds

      - name: Wait for 192.168.50.1 to be Accessible
        continue-on-error: true # Allow the step to continue even if the script fails
        run: |
          start_time=$(date +%s)
          end_time=$((start_time + $MAX_WAIT_TIME))

          echo "Waiting for $IP_ADDRESS to be accessible..."

          while true; do
            if ping -c 1 "$IP_ADDRESS" > /dev/null 2>&1; then
              echo "Router is accessible now. Continuing..."
              exit 0
            else
              current_time=$(date +%s)
              remaining_time=$((end_time - current_time))

              if [ $remaining_time -gt 0 ]; then
                echo "Router is not accessible yet. Please access this page to manually configure the router: https://github.com/tosin2013/demo-virt/blob/rhpds/demo.redhat.com/docs/step1.md"
                echo "Remaining time: $((remaining_time / 60)) minutes"
                sleep "$WAIT_INTERVAL"
              else
                echo "Timeout reached. Router is still not accessible."
                exit 0  # Exit with non-error code to avoid immediate failure of the job
              fi
            fi
          done

      - name: Validate Router Deployment
        if: ${{ success() }}
        run: |
          if ! ping -c 1 $IP_ADDRESS > /dev/null 2>&1; then
            echo "Router deployment failed. Manual intervention required."
            exit 1 # Fail only if the router is not accessible at this point
          else
            echo "Router is accessible."
          fi

  configre-oc-binary:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    if: ${{ inputs.action == 'create' }}
    needs: wait-for-vyos-route # Ensure the previous job runs first
    steps:
      - uses: actions/checkout@v4

      - name: Configure oc binary
        run: |
          #!/bin/bash

          # Check if oc is installed
          if ! command -v oc &>/dev/null; then
              echo "oc is not installed, downloading and running OpenShift package configuration script..."

              # Download the configuration script
              curl -OL https://raw.githubusercontent.com/tosin2013/openshift-4-deployment-notes/master/pre-steps/configure-openshift-packages.sh

              # Make the script executable
              chmod +x configure-openshift-packages.sh

              # Run the script with the -i flag
              sudo -E ./configure-openshift-packages.sh -i
          else
              echo "oc is already installed, skipping OpenShift package configuration."
          fi

  deploy-openshift:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    if: ${{ inputs.action == 'create' }}
    needs: configre-oc-binary # Ensure the previous job runs first
    steps:
      - uses: actions/checkout@v4

      - name: Deploy OpenShift
        run: |
          #!/bin/bash
          export TARGET_SERVER=${{ env.TARGET_SERVER }}
          export VM_NAME=${{ env.VM_NAME }}
          export VM_PROFILE=${{ env.VM_NAME }}
          export ACTION=${{ env.ACTION }}
          export DEPLOY_OPENSHIFT=${{ env.DEPLOY_OPENSHIFT }}
          export COMMUNITY_VERSION=${{ env.COMMUNITY_VERSION }}
          export FOLDER_NAME=${{ env.FOLDER_NAME }}
          export ZONE_NAME=${{ env.ZONE_NAME }}
          cd /opt/kcli-pipelines/
          sudo -E ./deploy-vm.sh || exit $?

  download-openshift-forwarder:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    if: ${{ inputs.action == 'create' }}
    needs: deploy-openshift # Ensure the previous job runs first
    steps:
      - uses: actions/checkout@v4

      - name: Download openshift-forwarder
        run: |
          cat >/tmp/requirements.yml<<EOF
          ---
          roles:
            - name: openshift-forwarder
              src: https://github.com/tosin2013/openshift-forwarder.git
              version: main
          EOF

          ansible-galaxy install -r /tmp/requirements.yml --force -vv

  configure-openshift-forwarder:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    if: ${{ inputs.action == 'create' }}
    needs: download-openshift-forwarder # Ensure the previous job runs first
    steps:
      - uses: actions/checkout@v4

      - name: Configure openshift-forwarder
        run: |
          #!/bin/bash

          cd /opt/openshift-agent-install

          # Path to the original YAML file
          input_file="examples/@param:FOLDER_NAME@/nodes.yml"

          # Output directory
          output_dir="$HOME/vars"
          mkdir -p "$output_dir"

          # Extract control plane and app node IPs using yq
          control_plane_ips=($(yq e '.nodes[] | select(.role == "master") | .networkConfig.interfaces[] | select(.name == "bond0.1924").ipv4.address[].ip' "$input_file"))
          app_node_ips=($(yq e '.nodes[] | select(.role == "worker") | .networkConfig.interfaces[] | select(.name == "bond0.1924").ipv4.address[].ip' "$input_file"))

          # Create the Ansible playbook
          cat > /tmp/openshift-forwarder.yml <<EOF
          - hosts: localhost
            become: true
            roles:
              - openshift-forwarder
          EOF

          # Start writing the output vars file
          cat > "$output_dir/vars.yml" <<EOF
          ---
          # defaults file for openshift-forwarder
          haproxy_log_address: "127.0.0.1"
          haproxy_chroot_directory: ""
          haproxy_pidfile: "/var/run/haproxy.pid"
          haproxy_max_connections: 4000
          haproxy_stats_socket: "/var/lib/haproxy/stats"
          defaults:
            retries: 3
            timeout:
              http_request: "10s"
              queue: "1m"
              connect: "10s"
              client: "1m"
              server: "1m"
              http_keep_alive: "10s"
              check: "10s"
            max_connections: 3000
          masters:
          EOF

          # Add control plane IPs to the vars file
          for ip in "${control_plane_ips[@]}"; do
              echo "  - ip: \"$ip\"" >> "$output_dir/vars.yml"
          done

          # Add worker IPs to the vars file
          if [ ${#app_node_ips[@]} -gt 0 ]; then
              echo "workers:" >> "$output_dir/vars.yml"
              for ip in "${app_node_ips[@]}"; do
                  echo "  - ip: \"$ip\"" >> "$output_dir/vars.yml"
              done
          elif [ ${#app_node_ips[@]} -eq 0 ]; then
              echo "workers:" >> "$output_dir/vars.yml"
              for ip in "${control_plane_ips[@]}"; do
                  echo "  - ip: \"$ip\"" >> "$output_dir/vars.yml"
              done
          fi

          # Run the Ansible playbook if the action is not "delete"
          if [ "@param:ACTION@" != "delete" ]; then
              ansible-playbook /tmp/openshift-forwarder.yml --extra-vars "@$output_dir/vars.yml" -e "ansible_python_interpreter=/usr/libexec/platform-python" -v
          fi


  delete-openshift:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    if: ${{ inputs.action == 'delete' }}
    steps:
      - uses: actions/checkout@v4

      - name: Deploy OpenShift
        run: |
          #!/bin/bash
          export TARGET_SERVER=${{ env.TARGET_SERVER }}
          export VM_NAME=${{ env.VM_NAME }}
          export VM_PROFILE=${{ env.VM_NAME }}
          export ACTION=${{ env.ACTION }}
          export DEPLOY_OPENSHIFT=${{ env.DEPLOY_OPENSHIFT }}
          export COMMUNITY_VERSION=${{ env.COMMUNITY_VERSION }}
          export FOLDER_NAME=${{ env.FOLDER_NAME }}
          export ZONE_NAME=${{ env.ZONE_NAME }}
          cd /opt/kcli-pipelines/
          sudo -E ./deploy-vm.sh || exit $?


  delete-dns-server:
    runs-on: ${{ inputs.runner_tag }} # Use the runner name at the job level
    timeout-minutes: 45
    needs: delete-openshift
    if: ${{ inputs.action == 'delete' }}
    steps:
      - uses: actions/checkout@v4

      - name: Deploying OpenShift Agent-Based Installer
        run: |
          export VM_NAME=freeipa
          export VM_PROFILE=freeipa
          export ACTION=${{ env.ACTION }}
          export COMMUNITY_VERSION=${{ env.COMMUNITY_VERSION }}

          CLUSTER_FILE_PATH="/opt/openshift-agent-install/examples/${{ env.FOLDER_NAME }}/cluster.yml"

          if [ -n "${{ env.ZONE_NAME }}" ]; then
            export DOMAIN="${{ env.GUID }}.${{ env.ZONE_NAME }}"
            sudo -E yq e -i ".domain = \"$DOMAIN\"" /opt/qubinode_navigator/inventories/"${{ inputs.target_server }}"/group_vars/all.yml
            sudo -E yq e -i ".base_domain = \"$DOMAIN\"" "${CLUSTER_FILE_PATH}"
            sudo -E yq e -i ".dns_search_domains[0] = \"$DOMAIN\"" "${CLUSTER_FILE_PATH}"
            sudo -E yq e -i "del(.dns_search_domains[1])" "${CLUSTER_FILE_PATH}"
          else
            DOMAIN=$(sudo -E yq eval '.domain' "${ANSIBLE_ALL_VARIABLES}")
          fi

          if [ "${ACTION}" == "create" ]; then
            cd /opt/kcli-pipelines/
            ./deploy-vm.sh
          fi